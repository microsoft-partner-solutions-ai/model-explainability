{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy web service with model explanations"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.model import Model\n",
        "from azureml.train.automl.run import AutoMLRun\n",
        "from automl.client.core.common.constants import MODEL_PATH"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1616180493638
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Connect to workspace and retrieve best AutoML run"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616180365344
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = ws.experiments['<experiment_name>']\n",
        "automl_run = AutoMLRun(experiment, run_id = '<AutoML_run_ID>') # use top level run id\n",
        "best_run = automl_run.get_best_child() # retrieve best child run by selected metric"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Received unrecognized parameter test_size\n",
            "WARNING:root:Received unrecognized parameter test_size\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616180368031
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Register model or retrieve previously registered model\n",
        "* Run 1st cell to register model \n",
        "OR\n",
        "* Run 2nd cell if model is already registered"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to register model to workspace\n",
        "model = best_run.register_model(model_name='<model_name>', model_path='outputs/model.pkl')"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616180381736
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell if model is already registered to workspace\n",
        "model = Model(ws, '<model_name>')"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616180503462
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get scoring explainer from workspace\n",
        "scoring_explainer = Model(ws, 'scoring_explainer')  # explainer should already be registered to workspace from 'explain.ipynb' notebook"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616180515387
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write scoring script"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile score.py\n",
        "import joblib\n",
        "import json\n",
        "import pandas as pd\n",
        "from azureml.core.model import Model\n",
        "from azureml.train.automl.runtime.automl_explain_utilities import automl_setup_model_explanations\n",
        "\n",
        "\n",
        "def init():\n",
        "    global model\n",
        "    global scoring_explainer\n",
        "\n",
        "    # Retrieve the path to the model file using the model name\n",
        "    model_path = Model.get_model_path('<model_name>')\n",
        "    scoring_explainer_path = Model.get_model_path('scoring_explainer')\n",
        "\n",
        "    # Load model and explainer\n",
        "    model = joblib.load(model_path)\n",
        "    scoring_explainer = joblib.load(scoring_explainer_path)\n",
        "\n",
        "\n",
        "def run(raw_data):\n",
        "    data = pd.read_json(raw_data)\n",
        "\n",
        "    # Make prediction\n",
        "    pred = model.predict(data)\n",
        "\n",
        "    # Get raw feature importance values\n",
        "    automl_explainer_setup_obj = automl_setup_model_explanations(model, X_test=data, task='classification')\n",
        "    raw_local_importance_values = scoring_explainer.explain(automl_explainer_setup_obj.X_test_transform, get_raw=True)\n",
        "\n",
        "    # Combine explanations with feature names, reverse sorted by importance score\n",
        "    num_records = data.shape[0]\n",
        "    explanations = []\n",
        "    for i in range(num_records):\n",
        "        exp_dict = dict(zip(automl_explainer_setup_obj.raw_feature_names,raw_local_importance_values[i]))\n",
        "        sorted_exp_dict = dict(sorted(exp_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "        explanations.append(sorted_exp_dict)\n",
        "\n",
        "    return {\"result\": pred.tolist(), \"explanations\": explanations}\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting score.py\n"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deploy model & explanations web service"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.webservice import Webservice\n",
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.core.model import Model, InferenceConfig\n",
        "from azureml.core.environment import Environment\n",
        "\n",
        "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1,\n",
        "                                               memory_gb=1,\n",
        "                                               description='Get predictions & explanations')\n",
        "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"conda_env.yml\")\n",
        "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n",
        "\n",
        "# Use configs and models generated above\n",
        "service = Model.deploy(ws,\n",
        "                       '<endpoint_name>',\n",
        "                       [model, scoring_explainer],\n",
        "                       inference_config,\n",
        "                       aciconfig)\n",
        "service.wait_for_deployment(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
            "Running...................................................\n",
            "Succeeded\n",
            "ACI service creation operation finished, operation \"Succeeded\"\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616180856589
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}